{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d02fe1b",
   "metadata": {},
   "source": [
    "## Triton Server APIs\n",
    "1. ### [Version](#1.-Version)\n",
    "    - [`GET /v2`](#GET-/v2)\n",
    "\n",
    "2. ### [Health Check](#2.-Health-Check)\n",
    "    - [`GET /v2/health/live`](#GET-/v2/health/live)\n",
    "    - [`GET /v2/health/ready`](#GET-/v2/health/ready)\n",
    "    - [`GET /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/ready`](#GET-/v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/ready)\n",
    "\n",
    "3. ### [Model Repository](#3.-Model-Repository)\n",
    "    - [`POST /v2/repository/index`](#POST-/v2/repository/index)\n",
    "    - [`POST /v2/repository/models/${MODEL_NAME}/load`](#POST-/v2/repository/models/${MODEL_NAME}/load)\n",
    "    \n",
    "    - [`POST /v2/repository/models/${MODEL_NAME}/unload`](#POST-/v2/repository/models/${MODEL_NAME}/unload)\n",
    "\n",
    "4. ### [Model](#4.-Model)\n",
    "    - [`GET /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/config`](#GET-/v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/config)\n",
    "    - [`GET /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/stats`](#GET-/v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/stats)\n",
    "    - [`POST /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/infer`](#POST-/v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/infer)\n",
    "        - [`models/add_sub`](#POST-/v2/models/add_sub[/versions/${MODEL_VERSION}]/infer)\n",
    "        \n",
    "        - [`models/inception_graphdef`](#POST-/v2/models/inception_graphdef[/versions/${MODEL_VERSION}]/infer)\n",
    "        \n",
    "        - [`models/resnet18_onnx`](#POST-/v2/models/resnet18_onnx[/versions/${MODEL_VERSION}]/infer)\n",
    "    \n",
    "5. ### [GPU Memory](#5.-GPU-Memory)\n",
    "    - [nvidia-smi](#nvidia-smi)\n",
    "\n",
    "6. ### [Prometheus Metrics](#6.-GPU-Memory)\n",
    "    - []()\n",
    "\n",
    "7. ### [Stability Tests](#7.-Stability-Tests)\n",
    "    - [Load & Unload Tests](#Stability-Tests-/-Load-&-Unload-Tests)\n",
    "        - [One Model, Many Versions](#Stability-Tests-/-Load-&-Unload-Tests-/-One-Model,-Many-Versions)\n",
    "        - [Many Models, One Version per Model](#Stability-Tests-/-Load-&-Unload-Tests-/-Many-Models,-One-Version-per-Model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f6ab27",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179ffe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IP = '10.78.26.241'\n",
    "HTTP_URL    = IP + ':9000'  # HTTP Service\n",
    "GRPC_URL    = IP + ':9001'  # GRPC Inference Service\n",
    "METRICS_URL = IP + ':9002'  # Metrics Service\n",
    "\n",
    "# short URL\n",
    "URL = HTTP_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b08e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c63b589",
   "metadata": {},
   "source": [
    "## 1. Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e65e77",
   "metadata": {},
   "source": [
    "### `GET /v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "4a601eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   254  100   254    0     0   253k      0 --:--:-- --:--:-- --:--:--  248k\r\n",
      "\u001b[1;39m{\r\n",
      "  \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"triton\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"version\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"2.11.0\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"extensions\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\r\n",
      "    \u001b[0;32m\"classification\"\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0;32m\"sequence\"\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0;32m\"model_repository\"\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0;32m\"model_repository(unload_dependents)\"\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0;32m\"schedule_policy\"\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0;32m\"model_configuration\"\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0;32m\"system_shared_memory\"\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0;32m\"cuda_shared_memory\"\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0;32m\"binary_tensor_data\"\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0;32m\"statistics\"\u001b[0m\u001b[1;39m\r\n",
      "  \u001b[1;39m]\u001b[0m\u001b[1;39m\r\n",
      "\u001b[1;39m}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!curl $URL/v2 | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d859a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eccced75",
   "metadata": {},
   "source": [
    "## 2. Health Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ddb87",
   "metadata": {},
   "source": [
    "### `GET /v2/health/live`\n",
    "- Failed:\n",
    "  ```\n",
    "  curl: (7) Failed to connect to 10.78.26.241 port 9000: Connection refused\n",
    "  ```\n",
    "- Successful\n",
    "  <br>200 OK, empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "364512f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl $URL/v2/health/live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "f89fbb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.78.26.241...\r\n",
      "* Connected to 10.78.26.241 (10.78.26.241) port 9000 (#0)\r\n",
      "> GET /v2/health/live HTTP/1.1\r",
      "\r\n",
      "> Host: 10.78.26.241:9000\r",
      "\r\n",
      "> User-Agent: curl/7.47.0\r",
      "\r\n",
      "> Accept: */*\r",
      "\r\n",
      "> \r",
      "\r\n",
      "< HTTP/1.1 200 OK\r",
      "\r\n",
      "< Content-Length: 0\r",
      "\r\n",
      "< Content-Type: text/plain\r",
      "\r\n",
      "< \r",
      "\r\n",
      "* Connection #0 to host 10.78.26.241 left intact\r\n"
     ]
    }
   ],
   "source": [
    "!curl -v $URL/v2/health/live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd70c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc905aeb",
   "metadata": {},
   "source": [
    "### `GET /v2/health/ready`\n",
    "- Failed:\n",
    "  ```\n",
    "  curl: (7) Failed to connect to 10.78.26.241 port 9000: Connection refused\n",
    "  ```\n",
    "- Successful\n",
    "  <br>200 OK, empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "60f690dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl $URL/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "76b8e5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.78.26.241...\r\n",
      "* Connected to 10.78.26.241 (10.78.26.241) port 9000 (#0)\r\n",
      "> GET /v2/health/ready HTTP/1.1\r",
      "\r\n",
      "> Host: 10.78.26.241:9000\r",
      "\r\n",
      "> User-Agent: curl/7.47.0\r",
      "\r\n",
      "> Accept: */*\r",
      "\r\n",
      "> \r",
      "\r\n",
      "< HTTP/1.1 200 OK\r",
      "\r\n",
      "< Content-Length: 0\r",
      "\r\n",
      "< Content-Type: text/plain\r",
      "\r\n",
      "< \r",
      "\r\n",
      "* Connection #0 to host 10.78.26.241 left intact\r\n"
     ]
    }
   ],
   "source": [
    "!curl -v $URL/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4814e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a209f6c1",
   "metadata": {},
   "source": [
    "### `GET /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/ready`\n",
    "- Not ready:\n",
    "  <br>400 Bad Request\n",
    "- Ready\n",
    "  <br>200 OK, empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1802dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'add_sub'\n",
    "MODEL_VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "81b784f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.78.26.241...\r\n",
      "* Connected to 10.78.26.241 (10.78.26.241) port 9000 (#0)\r\n",
      "> GET /v2/models/inception_graphdef/ready HTTP/1.1\r",
      "\r\n",
      "> Host: 10.78.26.241:9000\r",
      "\r\n",
      "> User-Agent: curl/7.47.0\r",
      "\r\n",
      "> Accept: */*\r",
      "\r\n",
      "> \r",
      "\r\n",
      "< HTTP/1.1 400 Bad Request\r",
      "\r\n",
      "< Content-Length: 0\r",
      "\r\n",
      "< Content-Type: text/plain\r",
      "\r\n",
      "< \r",
      "\r\n",
      "* Connection #0 to host 10.78.26.241 left intact\r\n"
     ]
    }
   ],
   "source": [
    "!curl -v $URL/v2/models/$MODEL_NAME/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05511d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.78.26.241...\r\n",
      "* Connected to 10.78.26.241 (10.78.26.241) port 9000 (#0)\r\n",
      "> GET /v2/models/add_sub/versions/1/ready HTTP/1.1\r",
      "\r\n",
      "> Host: 10.78.26.241:9000\r",
      "\r\n",
      "> User-Agent: curl/7.47.0\r",
      "\r\n",
      "> Accept: */*\r",
      "\r\n",
      "> \r",
      "\r\n",
      "< HTTP/1.1 200 OK\r",
      "\r\n",
      "< Content-Length: 0\r",
      "\r\n",
      "< Content-Type: text/plain\r",
      "\r\n",
      "< \r",
      "\r\n",
      "* Connection #0 to host 10.78.26.241 left intact\r\n"
     ]
    }
   ],
   "source": [
    "!curl -v $URL/v2/models/$MODEL_NAME/versions/$MODEL_VERSION/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c466c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a27ffb8",
   "metadata": {},
   "source": [
    "## 3. Model Repository\n",
    "- 參考資料\n",
    "  - [Model Repository Extension](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_repository.md#httprest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067daa89",
   "metadata": {},
   "source": [
    "### `POST /v2/repository/index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2516410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.78.26.241...\r\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Connected to 10.78.26.241 (10.78.26.241) port 9000 (#0)\r\n",
      "> POST /v2/repository/index HTTP/1.1\r",
      "\r\n",
      "> Host: 10.78.26.241:9000\r",
      "\r\n",
      "> User-Agent: curl/7.47.0\r",
      "\r\n",
      "> Accept: */*\r",
      "\r\n",
      "> \r",
      "\r\n",
      "< HTTP/1.1 200 OK\r",
      "\r\n",
      "< Content-Type: application/json\r",
      "\r\n",
      "< Content-Length: 105\r",
      "\r\n",
      "< \r",
      "\r\n",
      "{ [105 bytes data]\r\n",
      "\r",
      "100   105  100   105    0     0  86848      0 --:--:-- --:--:-- --:--:--  102k\r\n",
      "* Connection #0 to host 10.78.26.241 left intact\r\n",
      "\u001b[1;39m[\r\n",
      "  \u001b[1;39m{\r\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"add_sub\"\u001b[0m\u001b[1;39m\r\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[1;39m{\r\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"inception_graphdef\"\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0m\u001b[34;1m\"version\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"2\"\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0m\u001b[34;1m\"state\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"READY\"\u001b[0m\u001b[1;39m\r\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[1;39m{\r\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"resnet18_onnx\"\u001b[0m\u001b[1;39m\r\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\r\n",
      "\u001b[1;39m]\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!curl -v -X POST $URL/v2/repository/index | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88021fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37906fcc",
   "metadata": {},
   "source": [
    "### Load model names from `/v2/repository/index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fd053874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model index: [\n",
      "    {\n",
      "        \"name\": \"add_sub\",\n",
      "        \"version\": \"1\",\n",
      "        \"state\": \"UNAVAILABLE\",\n",
      "        \"reason\": \"unloaded\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"inception_graphdef\",\n",
      "        \"version\": \"1\",\n",
      "        \"state\": \"UNAVAILABLE\",\n",
      "        \"reason\": \"unloaded\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"resnet18_onnx\",\n",
      "        \"version\": \"1\",\n",
      "        \"state\": \"UNAVAILABLE\",\n",
      "        \"reason\": \"unloaded\"\n",
      "    }\n",
      "]\n",
      "model names: ['add_sub', 'inception_graphdef', 'resnet18_onnx']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# load model index from 'v2/repository/index'\n",
    "def get_model_index():\n",
    "    responses = !curl -X POST $URL/v2/repository/index\n",
    "    if len(responses) < 5:\n",
    "        raise Exception('\\n' + '\\n'.join(responses))\n",
    "    response = responses[5] # skip the head info\n",
    "    \n",
    "    # response:\n",
    "    #   [{\"name\":\"add_sub\",\"version\":\"1\",\"state\":\"READY\"}, ...]\n",
    "    model_index = json.loads(response)\n",
    "    return model_index\n",
    "\n",
    "# load model names from 'v2/repository/index'\n",
    "def get_model_names():\n",
    "    model_index = get_model_index()\n",
    "    \n",
    "    model_names = []\n",
    "    for model in model_index:\n",
    "        if model['name'] not in model_names:\n",
    "            model_names.append(model['name'])\n",
    "    \n",
    "    return model_names\n",
    "\n",
    "\n",
    "print(\"model index:\", json.dumps(get_model_index(), indent=4, sort_keys=False))\n",
    "print(\"model names:\", get_model_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "9326e0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to 10.78.26.241 port 9000: Connection refused\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1943a393",
   "metadata": {},
   "source": [
    "### `POST /v2/repository/models/${MODEL_NAME}/load`\n",
    "- Failed:\n",
    "  - no response\n",
    "    <br>The state has been 'UNLOADING'.\n",
    "  - use `--gpus='\"device=0\"'`, but assign `1` to `instance_group.gpu`\n",
    "    - server log: ```unsupported gpu id 1```\n",
    "    - client log: ```{\"error\":\"failed to load 'resnet18_onnx', no version is available\"}```\n",
    "  - out of shared memory\n",
    "    - server log:\n",
    "      ```\n",
    "      model_repository_manager.cc:1215] failed to load '${MODEL_NAME}' version ${MODEL_VERSION}: Internal: Unable to initialize shared memory key '/{MODEL_NAME}_0_CPU_0' to requested size (67108864 bytes). If you are running Triton inside docker, use '--shm-size' flag to control the shared memory region size. Each Python backend model instance requires at least 64MBs of shared memory. Flag '--shm-size=5G' should be sufficient for common usecases. Error: No such file or directory\n",
    "      ```\n",
    "- Successful\n",
    "  <br>200 OK, empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2b7b6835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.78.26.241...\n",
      "* Connected to 10.78.26.241 (10.78.26.241) port 9000 (#0)\n",
      "> POST /v2/repository/models/resnet18_onnx/load HTTP/1.1\n",
      "> Host: 10.78.26.241:9000\n",
      "> User-Agent: curl/7.47.0\n",
      "> Accept: */*\n",
      "> \n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 0\n",
      "< \n",
      "* Connection #0 to host 10.78.26.241 left intact\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = ['add_sub', 'inception_graphdef', 'resnet18_onnx'][2]\n",
    "\n",
    "!curl -v -X POST $URL/v2/repository/models/$MODEL_NAME/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f42e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e2167a1",
   "metadata": {},
   "source": [
    "### `POST /v2/repository/models/${MODEL_NAME}/unload`\n",
    "- Failed:\n",
    "  \n",
    "- Successful\n",
    "  <br>200 OK, empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "dd5527d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.78.26.241...\r\n",
      "* Connected to 10.78.26.241 (10.78.26.241) port 9000 (#0)\r\n",
      "> POST /v2/repository/models/resnet18_onnx/unload HTTP/1.1\r",
      "\r\n",
      "> Host: 10.78.26.241:9000\r",
      "\r\n",
      "> User-Agent: curl/7.47.0\r",
      "\r\n",
      "> Accept: */*\r",
      "\r\n",
      "> \r",
      "\r\n",
      "< HTTP/1.1 200 OK\r",
      "\r\n",
      "< Content-Type: application/json\r",
      "\r\n",
      "< Content-Length: 0\r",
      "\r\n",
      "< \r",
      "\r\n",
      "* Connection #0 to host 10.78.26.241 left intact\r\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = ['add_sub', 'inception_graphdef', 'resnet18_onnx'][2]\n",
    "\n",
    "!curl -v -X POST $URL/v2/repository/models/$MODEL_NAME/unload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e315f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d41148d2",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74141e2b",
   "metadata": {},
   "source": [
    "### `GET /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/config`\n",
    "- Not loaded yet (state: empty)\n",
    "  - `{ \"error\": \"Request for unknown model: 'inception_graphdef' is not found\" }`\n",
    "- Not ready: (state: \"unloaded\")\n",
    "  - `{ \"error\": \"Request for unknown model: '${MODEL_NAME}' has no available versions\" }`\n",
    "  - `{ \"error\": \"Request for unknown model: '${MODEL_NAME}' version ${MODEL_VERSION} is not at ready state\" }`\n",
    "- Ready (state: \"READY\")\n",
    "  <br>200 OK, json-result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d8b8866e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.78.26.241...\r\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Connected to 10.78.26.241 (10.78.26.241) port 9000 (#0)\r\n",
      "> GET /v2/models/resnet18_onnx/config HTTP/1.1\r",
      "\r\n",
      "> Host: 10.78.26.241:9000\r",
      "\r\n",
      "> User-Agent: curl/7.47.0\r",
      "\r\n",
      "> Accept: */*\r",
      "\r\n",
      "> \r",
      "\r\n",
      "< HTTP/1.1 200 OK\r",
      "\r\n",
      "< Content-Type: application/json\r",
      "\r\n",
      "< Content-Length: 923\r",
      "\r\n",
      "< \r",
      "\r\n",
      "{ [923 bytes data]\r\n",
      "\r",
      "100   923  100   923    0     0   592k      0 --:--:-- --:--:-- --:--:--  901k\r\n",
      "* Connection #0 to host 10.78.26.241 left intact\r\n",
      "\u001b[1;39m{\r\n",
      "  \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"resnet18_onnx\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"platform\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"onnxruntime_onnx\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"backend\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"onnxruntime\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"version_policy\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "    \u001b[0m\u001b[34;1m\"all\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{}\u001b[0m\u001b[1;39m\r\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"max_batch_size\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"input\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\r\n",
      "    \u001b[1;39m{\r\n",
      "      \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"data\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"data_type\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"TYPE_FP32\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"format\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"FORMAT_NCHW\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"dims\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\r\n",
      "        \u001b[0;39m3\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0;39m224\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0;39m224\u001b[0m\u001b[1;39m\r\n",
      "      \u001b[1;39m]\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"reshape\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "        \u001b[0m\u001b[34;1m\"shape\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\r\n",
      "          \u001b[0;39m1\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0;39m3\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0;39m224\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0;39m224\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m]\u001b[0m\u001b[1;39m\r\n",
      "      \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"is_shape_tensor\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mfalse\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"allow_ragged_batch\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mfalse\u001b[0m\u001b[1;39m\r\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m\r\n",
      "  \u001b[1;39m]\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"output\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\r\n",
      "    \u001b[1;39m{\r\n",
      "      \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"resnetv22_dense0_fwd\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"data_type\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"TYPE_FP32\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"dims\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\r\n",
      "        \u001b[0;39m1\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0;39m1000\u001b[0m\u001b[1;39m\r\n",
      "      \u001b[1;39m]\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"label_filename\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"synset.txt\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"is_shape_tensor\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mfalse\u001b[0m\u001b[1;39m\r\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m\r\n",
      "  \u001b[1;39m]\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"batch_input\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[]\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"batch_output\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[]\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"optimization\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "    \u001b[0m\u001b[34;1m\"priority\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"PRIORITY_DEFAULT\"\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0m\u001b[34;1m\"input_pinned_memory\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "      \u001b[0m\u001b[34;1m\"enable\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mtrue\u001b[0m\u001b[1;39m\r\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0m\u001b[34;1m\"output_pinned_memory\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "      \u001b[0m\u001b[34;1m\"enable\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mtrue\u001b[0m\u001b[1;39m\r\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0m\u001b[34;1m\"gather_kernel_buffer_threshold\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[0m\u001b[34;1m\"eager_batching\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mfalse\u001b[0m\u001b[1;39m\r\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"instance_group\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\r\n",
      "    \u001b[1;39m{\r\n",
      "      \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"resnet18_onnx_0\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"kind\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"KIND_GPU\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"gpus\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\r\n",
      "        \u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "      \u001b[1;39m]\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"secondary_devices\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[]\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"profile\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[]\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"passive\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mfalse\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"host_policy\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"\"\u001b[0m\u001b[1;39m\r\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m\r\n",
      "  \u001b[1;39m]\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"default_model_filename\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"model.onnx\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"cc_model_filenames\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{}\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"metric_tags\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{}\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"parameters\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{}\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"model_warmup\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[]\u001b[0m\u001b[1;39m\r\n",
      "\u001b[1;39m}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = ['add_sub', 'inception_graphdef', 'resnet18_onnx'][2]\n",
    "MODEL_VERSION = 2\n",
    "\n",
    "# current version\n",
    "!curl -v $URL/v2/models/$MODEL_NAME/config | jq\n",
    "\n",
    "# specified version\n",
    "#!curl -v $URL/v2/models/$MODEL_NAME/versions/$MODEL_VERSION/config | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86175d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62a73481",
   "metadata": {},
   "source": [
    "### `GET /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/stats`\n",
    "- Not ready:\n",
    "  <br>`{ \"error\": \"requested model '${MODEL_NAME}' is not available\" }`\n",
    "- Ready\n",
    "  <br>200 OK, json-result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e8ba2794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.78.26.241...\r\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Connected to 10.78.26.241 (10.78.26.241) port 9000 (#0)\r\n",
      "> GET /v2/models/resnet18_onnx/stats HTTP/1.1\r",
      "\r\n",
      "> Host: 10.78.26.241:9000\r",
      "\r\n",
      "> User-Agent: curl/7.47.0\r",
      "\r\n",
      "> Accept: */*\r",
      "\r\n",
      "> \r",
      "\r\n",
      "< HTTP/1.1 200 OK\r",
      "\r\n",
      "< Content-Type: application/json\r",
      "\r\n",
      "< Content-Length: 986\r",
      "\r\n",
      "< \r",
      "\r\n",
      "{ [986 bytes data]\r\n",
      "\r",
      "100   986  100   986    0     0   947k      0 --:--:-- --:--:-- --:--:--  962k\r\n",
      "* Connection #0 to host 10.78.26.241 left intact\r\n",
      "\u001b[1;39m{\r\n",
      "  \u001b[0m\u001b[34;1m\"model_stats\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\r\n",
      "    \u001b[1;39m{\r\n",
      "      \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"resnet18_onnx\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"version\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"last_inference\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"inference_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"execution_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"inference_stats\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "        \u001b[0m\u001b[34;1m\"success\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"fail\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"queue\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"compute_input\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"compute_infer\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"compute_output\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m\r\n",
      "      \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"batch_stats\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[]\u001b[0m\u001b[1;39m\r\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[1;39m{\r\n",
      "      \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"resnet18_onnx\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"version\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"2\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"last_inference\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"inference_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"execution_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"inference_stats\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "        \u001b[0m\u001b[34;1m\"success\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"fail\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"queue\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"compute_input\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"compute_infer\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"compute_output\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m\r\n",
      "      \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"batch_stats\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[]\u001b[0m\u001b[1;39m\r\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "    \u001b[1;39m{\r\n",
      "      \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"resnet18_onnx\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"version\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"3\"\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"last_inference\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"inference_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"execution_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"inference_stats\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "        \u001b[0m\u001b[34;1m\"success\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"fail\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"queue\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"compute_input\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"compute_infer\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "        \u001b[0m\u001b[34;1m\"compute_output\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "          \u001b[0m\u001b[34;1m\"count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "          \u001b[0m\u001b[34;1m\"ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m\r\n",
      "        \u001b[1;39m}\u001b[0m\u001b[1;39m\r\n",
      "      \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "      \u001b[0m\u001b[34;1m\"batch_stats\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[]\u001b[0m\u001b[1;39m\r\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m\r\n",
      "  \u001b[1;39m]\u001b[0m\u001b[1;39m\r\n",
      "\u001b[1;39m}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = ['add_sub', 'inception_graphdef', 'resnet18_onnx'][2]\n",
    "MODEL_VERSION = 2\n",
    "\n",
    "# current version\n",
    "!curl -v $URL/v2/models/$MODEL_NAME/stats | jq\n",
    "\n",
    "# specified version\n",
    "#!curl -v $URL/v2/models/$MODEL_NAME/versions/$MODEL_VERSION/stats | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bbdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5203c59",
   "metadata": {},
   "source": [
    "### `POST /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/infer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae027bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_command(cmd):\n",
    "    #print('cmd:', cmd, '\\n')\n",
    "    \n",
    "    # method1\n",
    "    ! $cmd\n",
    "    \n",
    "    # method2: without the result\n",
    "    '''\n",
    "    import os\n",
    "    os.system(cmd)\n",
    "    '''\n",
    "    \n",
    "    # method3: with the result\n",
    "    '''\n",
    "    import subprocess\n",
    "    output = subprocess.check_output(cmd, shell=True)\n",
    "    print(output.decode('utf-8'))\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f02a3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_image_client_py(model_name, model_version=None, image_filename='images/mug.jpg', \n",
    "                            verbose=False, no_output=False):\n",
    "    cmd = '''\n",
    "        python3.6 \\\n",
    "            \"image_client.py\" \\\n",
    "            -u {url} \\\n",
    "            -m {model_name} \\\n",
    "            {model_version} \\\n",
    "            -s INCEPTION \\\n",
    "            {image_filename} \\\n",
    "        '''\n",
    "    cmd = cmd.format(\n",
    "        url=URL, \n",
    "        model_name=model_name,\n",
    "        model_version=' -x ' + str(model_version) if model_version != None else '',\n",
    "        image_filename=image_filename\n",
    "    )\n",
    "    \n",
    "    if no_output:\n",
    "        cmd += ' 2>&1 > /dev/null'\n",
    "    \n",
    "    execute_command(cmd)\n",
    "    \n",
    "    if not no_output:\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6074a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fc4e49e",
   "metadata": {},
   "source": [
    "### `POST /v2/models/add_sub[/versions/${MODEL_VERSION}]/infer`\n",
    "\n",
    "- shell\n",
    "```\n",
    "curl -X POST 10.78.26.241:9000/v2/models/add_sub/infer \\\n",
    "  --data '{\"inputs\": [{\"name\": \"INPUT0\", \"shape\": [4], \"datatype\": \"FP32\", \"data\": [1, 2, 3, 4]}, {\"name\": \"INPUT1\", \"shape\": [4], \"datatype\": \"FP32\", \"data\": [1, 1, 1, 1]}]}'\n",
    "```\n",
    "or\n",
    "```\n",
    "curl -X POST 10.78.26.241:9000/v2/models/add_sub/infer \\\n",
    "  --data \"{\\\"inputs\\\": [ \\\n",
    "    {\\\"name\\\": \\\"INPUT0\\\", \\\"shape\\\": [4], \\\"datatype\\\": \\\"FP32\\\", \\\"data\\\": [1, 2, 3, 4]}, \\\n",
    "    {\\\"name\\\": \\\"INPUT1\\\", \\\"shape\\\": [4], \\\"datatype\\\": \\\"FP32\\\", \\\"data\\\": [1, 1, 1, 1]} \\\n",
    "  ]}\"\n",
    "```\n",
    "\n",
    "- shell in notebook\n",
    "```\n",
    "!curl -X POST $URL/v2/models/add_sub/infer \\\n",
    "  --data \"{{\\\"inputs\\\": [ \\\n",
    "    {{ \\\"name\\\": \\\"INPUT0\\\", \\\"shape\\\": [4], \\\"datatype\\\": \\\"FP32\\\", \\\"data\\\": [1, 2, 3, 4] }}, \\\n",
    "    {{ \\\"name\\\": \\\"INPUT1\\\", \\\"shape\\\": [4], \\\"datatype\\\": \\\"FP32\\\", \\\"data\\\": [1, 1, 1, 1] }} \\\n",
    "    ]}}\"\n",
    "```\n",
    "    - `{`, `}`: need to repeat twice to escape special character\n",
    "    - use double quote instead of single quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "898d5e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_name\":\"add_sub\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"FP32\",\"shape\":[4],\"data\":[2.0,3.0,4.0,5.0]},{\"name\":\"OUTPUT1\",\"datatype\":\"FP32\",\"shape\":[4],\"data\":[0.0,1.0,2.0,3.0]}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def add_sub_infer(version=None, verbose=False, no_output=False):\n",
    "    data = {\n",
    "        'inputs': [\n",
    "            {\n",
    "                'name': 'INPUT0',\n",
    "                'shape': [4],\n",
    "                'datatype': 'FP32',\n",
    "                'data': [1, 2, 3, 4]\n",
    "            },\n",
    "            {\n",
    "                'name': 'INPUT1',\n",
    "                'shape': [4],\n",
    "                'datatype': 'FP32',\n",
    "                'data': [1, 1, 1, 1]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if version == None:\n",
    "        endpoint = URL + '/v2/models/add_sub/infer'\n",
    "    else:\n",
    "        endpoint = URL + '/v2/models/add_sub/versions/{}/infer'.format(version)\n",
    "    \n",
    "    cmd = \"curl -s {verbose} -X POST {endpoint} --data '{data}'\".format(\n",
    "        verbose='' if not verbose else '-v', \n",
    "        endpoint=endpoint, \n",
    "        data=json.dumps(data))\n",
    "    \n",
    "    if no_output:\n",
    "        cmd += ' 2>&1 > /dev/null'\n",
    "    \n",
    "    execute_command(cmd)\n",
    "    \n",
    "    if not no_output:\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "add_sub_infer(no_output=False)\n",
    "add_sub_infer(version=1,no_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50fb42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fcc2924",
   "metadata": {},
   "source": [
    "### `POST /v2/models/inception_graphdef[/versions/${MODEL_VERSION}]/infer`\n",
    "- python-based (via [image_client.py](https://github.com/triton-inference-server/client/blob/main/src/python/examples/image_client.py))\n",
    "  ```\n",
    "  python3.6  \"image_client.py\" \\\n",
    "    -u 10.78.26.241:9000 \\\n",
    "    -m inception_graphdef \\\n",
    "    -s INCEPTION \\\n",
    "    images/mug.jpg\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a03c35e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1, batch size 1\n",
      "    0.826453 (505) = COFFEE MUG\n",
      "PASS\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def inception_graphdef_infer(version=None, verbose=False, no_output=False):\n",
    "    execute_image_client_py(\n",
    "        model_name='inception_graphdef',\n",
    "        model_version=version,\n",
    "        verbose=verbose,\n",
    "        no_output=no_output\n",
    "    )\n",
    "\n",
    "inception_graphdef_infer(no_output=True)\n",
    "inception_graphdef_infer(version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913278d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19fb64e2",
   "metadata": {},
   "source": [
    "### `POST /v2/models/resnet18_onnx[/versions/${MODEL_VERSION}]/infer`\n",
    "- python-based (via [image_client.py](https://github.com/triton-inference-server/client/blob/main/src/python/examples/image_client.py))\n",
    "  ```\n",
    "  python3.6  \"image_client.py\" \\\n",
    "    -u 10.78.26.241:9000 \\\n",
    "    -m resnet18_onnx \\\n",
    "    -s INCEPTION \\\n",
    "    images/mug.jpg\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3a4ea125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to retrieve the metadata: Request for unknown model: 'resnet18_onnx' version 2 is not found\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def resnet18_onnx_infer(version=None, verbose=False, no_output=False):\n",
    "    execute_image_client_py(\n",
    "        model_name='resnet18_onnx',\n",
    "        model_version=version,\n",
    "        verbose=verbose,\n",
    "        no_output=no_output\n",
    "    )\n",
    "\n",
    "resnet18_onnx_infer(no_output=True)\n",
    "resnet18_onnx_infer(version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1429950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744bc98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c21af5c3",
   "metadata": {},
   "source": [
    "## 5. GPU Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61742c",
   "metadata": {},
   "source": [
    "### `nvidia-smi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "70141201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 26 11:03:33 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 27%   26C    P8     8W / 250W |   1096MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:03:00.0 Off |                  N/A |\n",
      "| 20%   31C    P8     8W / 250W |   1231MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:82:00.0 Off |                  N/A |\n",
      "| 40%   72C    P2    79W / 250W |   5311MiB / 11178MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:83:00.0 Off |                  N/A |\n",
      "| 27%   61C    P2    86W / 250W |   5181MiB / 11178MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1252744      C   deepstream-test1-app              373MiB |\n",
      "|    0   N/A  N/A   1487697      C   ./darknet                         147MiB |\n",
      "|    0   N/A  N/A   1554431      C   deepstream-test1-app              373MiB |\n",
      "|    0   N/A  N/A   3088550      C   tritonserver                      199MiB |\n",
      "|    1   N/A  N/A   2960901      C   tritonserver                     1229MiB |\n",
      "|    2   N/A  N/A   1487697      C   ./darknet                        5241MiB |\n",
      "|    3   N/A  N/A   1487697      C   ./darknet                        5179MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "02f5d472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input gpu-index, i.e. [0|1|2|3]:0\n",
      "Please input pid:638528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_gpu_memory(gpu, pid):\n",
    "    response = !nvidia-smi\n",
    "    \n",
    "    for line in response:\n",
    "        matcher = re.search('^\\|\\s+(\\d+)\\s+[^\\s]+\\s+[^\\s]+\\s+(\\d+).*?(\\d+)MiB\\s\\|$', line)\n",
    "        \n",
    "        if matcher != None:\n",
    "            info_gpu = matcher.group(1)\n",
    "            info_pid = matcher.group(2)\n",
    "            info_memory = matcher.group(3)\n",
    "            \n",
    "            if int(gpu) == int(info_gpu) and int(pid) == int(info_pid):\n",
    "                return int(info_memory)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "gpu = input('Please input gpu-index, i.e. [0|1|2|3]:')\n",
    "pid = input('Please input pid:')\n",
    "get_gpu_memory(gpu=gpu, pid=pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b6786922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789767d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3d83033",
   "metadata": {},
   "source": [
    "## 7. Stability Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284096bc",
   "metadata": {},
   "source": [
    "### Stability Tests / Load & Unload Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d211ec4",
   "metadata": {},
   "source": [
    "### Stability Tests / Load & Unload Tests / One Model, Many Versions\n",
    "> 只有一種模型,內含多個 version, 針對最後一個 version 重複 load / unload\n",
    "\n",
    "- `version_policy: { all { }}`\n",
    "- Server logs: (for example)\n",
    "  ```\n",
    "  loading: inception_graphdef:1\n",
    "  loading: inception_graphdef:2\n",
    "  loading: inception_graphdef:3\n",
    "  successfully loaded 'inception_graphdef' version 1\n",
    "  successfully loaded 'inception_graphdef' version 2\n",
    "  successfully loaded 'inception_graphdef' version 3\n",
    "  unloading: inception_graphdef:1\n",
    "  unloading: inception_graphdef:2\n",
    "  unloading: inception_graphdef:3\n",
    "  successfully unloaded 'inception_graphdef' version 2 (Does not guarantee the order)\n",
    "  successfully unloaded 'inception_graphdef' version 3\n",
    "  successfully unloaded 'inception_graphdef' version 1\n",
    "  ...\n",
    "  (repeated)\n",
    "  ```\n",
    "- Failed case:\n",
    "  - python-backend with many versions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "231b20d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input gpu-index, i.e. [0|1|2|3]: 0\n",
      "Please input gpu-pid: 751546\n",
      "Current GPU memory: 199MiB\n",
      "\n",
      "MODEL_NAMES: ['resnet18_onnx']\n",
      "\n",
      "Round: [199MiB] -> 0 -> [1017MiB] -> 1 -> [1017MiB] -> 2 -> [1017MiB] -> 3 -> [1017MiB] -> 4 -> [1017MiB] -> 5 -> [1017MiB] -> 6 -> [1017MiB] -> 7 -> [1017MiB] -> 8 -> [1017MiB] -> 9 -> [1017MiB] -> \n",
      "\n",
      "Final state:\n",
      "[\n",
      "    {\n",
      "        \"name\": \"resnet18_onnx\",\n",
      "        \"version\": \"1\",\n",
      "        \"state\": \"UNAVAILABLE\",\n",
      "        \"reason\": \"unloaded\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"resnet18_onnx\",\n",
      "        \"version\": \"2\",\n",
      "        \"state\": \"UNAVAILABLE\",\n",
      "        \"reason\": \"unloaded\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"resnet18_onnx\",\n",
      "        \"version\": \"3\",\n",
      "        \"state\": \"UNAVAILABLE\",\n",
      "        \"reason\": \"unloaded\"\n",
      "    }\n",
      "]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# get the following info from nvidia-smi\n",
    "gpu = input('Please input gpu-index, i.e. [0|1|2|3]: ')\n",
    "pid = input('Please input gpu-pid: ')\n",
    "print('Current GPU memory: %dMiB' % get_gpu_memory(gpu=gpu, pid=pid))\n",
    "print()\n",
    "\n",
    "MODEL_NAMES = get_model_names()\n",
    "print(\"MODEL_NAMES:\", MODEL_NAMES)\n",
    "print()\n",
    "\n",
    "MODEL_NAME = MODEL_NAMES[0]\n",
    "\n",
    "print('Round:', end=' ')\n",
    "print('[%dMiB] -> ' % get_gpu_memory(gpu=gpu, pid=pid), end='')\n",
    "for i in range(10):\n",
    "    print(i, end='')\n",
    "    !curl -X POST $URL/v2/repository/models/$MODEL_NAME/load\n",
    "    \n",
    "    # warmup\n",
    "    function_name = MODEL_NAME + '_infer'\n",
    "    globals()[function_name](no_output=True)\n",
    "\n",
    "    !curl -X POST $URL/v2/repository/models/$MODEL_NAME/unload\n",
    "    \n",
    "    print(' -> [%dMiB] -> ' % get_gpu_memory(gpu=gpu, pid=pid), end='')\n",
    "\n",
    "print('\\n')\n",
    "print('Final state:')\n",
    "print(json.dumps(get_model_index(), indent=4))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c518f059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb5b5e37",
   "metadata": {},
   "source": [
    "### Stability Tests / Load & Unload Tests / Many Models, One Version per Model\n",
    "> repository 放置多種模型(只含一個 version)\n",
    "> - 載入全部模型後, 反複 load / unload 某個模型\n",
    "\n",
    "\n",
    "TritonServer parameter:\n",
    "- mount path\n",
    "    ```\n",
    "    -v ~/tj_tsai/workspace/infra/triton_server/workspace/models_v1:/models\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8901a222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input gpu-index, i.e. [0|1|2|3]: 0\n",
      "Please input gpu-pid: 701226\n",
      "Current GPU memory: 199MiB\n",
      "\n",
      "MODEL_NAMES: ['add_sub', 'inception_graphdef', 'resnet18_onnx']\n",
      "\n",
      "loading all models:\n",
      ">> [199MiB] \n",
      "- [199MiB] -> add_sub -> [273MiB] \n",
      "- [273MiB] -> inception_graphdef -> [827MiB] \n",
      "- [827MiB] -> resnet18_onnx -> [1087MiB] \n",
      "<< [1087MiB] \n",
      "\n",
      "load & infer & unload for each round:\n",
      "------------------------------------------------------------\n",
      "Target model: add_sub\n",
      "Round: [1087MiB] -> 0 -> [1087MiB] -> 1 -> [1087MiB] -> 2 -> [1087MiB] -> 3 -> [1087MiB] -> 4 -> [1087MiB] -> 5 -> [1087MiB] -> 6 -> [1087MiB] -> 7 -> [1087MiB] -> 8 -> [1087MiB] -> 9 -> [1087MiB] -> \n",
      "\n",
      "Target model: inception_graphdef\n",
      "Round: [1087MiB] -> 0 -> [4497MiB] -> 1 -> [4497MiB] -> 2 -> [4497MiB] -> 3 -> [4497MiB] -> 4 -> [4497MiB] -> 5 -> [4497MiB] -> 6 -> [4497MiB] -> 7 -> [4497MiB] -> 8 -> [4497MiB] -> 9 -> [4497MiB] -> \n",
      "\n",
      "Target model: resnet18_onnx\n",
      "Round: [4497MiB] -> 0 -> [4413MiB] -> 1 -> [4413MiB] -> 2 -> [4413MiB] -> 3 -> [4413MiB] -> 4 -> [4413MiB] -> 5 -> [4413MiB] -> 6 -> [4413MiB] -> 7 -> [4413MiB] -> 8 -> [4413MiB] -> 9 -> [4413MiB] -> \n",
      "\n",
      "------------------------------------------------------------\n",
      "Final state:\n",
      "[\n",
      "    {\n",
      "        \"name\": \"add_sub\",\n",
      "        \"version\": \"1\",\n",
      "        \"state\": \"UNAVAILABLE\",\n",
      "        \"reason\": \"unloaded\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"inception_graphdef\",\n",
      "        \"version\": \"1\",\n",
      "        \"state\": \"UNAVAILABLE\",\n",
      "        \"reason\": \"unloaded\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"resnet18_onnx\",\n",
      "        \"version\": \"1\",\n",
      "        \"state\": \"UNAVAILABLE\",\n",
      "        \"reason\": \"unloaded\"\n",
      "    }\n",
      "]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# get the following info from nvidia-smi\n",
    "gpu = input('Please input gpu-index, i.e. [0|1|2|3]: ')\n",
    "pid = input('Please input gpu-pid: ')\n",
    "print('Current GPU memory: %dMiB' % get_gpu_memory(gpu=gpu, pid=pid))\n",
    "print()\n",
    "\n",
    "MODEL_NAMES = get_model_names()\n",
    "print(\"MODEL_NAMES:\", MODEL_NAMES)\n",
    "print()\n",
    "\n",
    "# STEP1: load all models\n",
    "print('loading all models:')\n",
    "print('>> ' '[%dMiB] ' % get_gpu_memory(gpu=gpu, pid=pid))\n",
    "for MODEL_NAME in MODEL_NAMES:\n",
    "    print('-', '[%dMiB] ->' % get_gpu_memory(gpu=gpu, pid=pid), MODEL_NAME, end=' ')\n",
    "    !curl -X POST $URL/v2/repository/models/$MODEL_NAME/load\n",
    "    print('-> [%dMiB] ' % get_gpu_memory(gpu=gpu, pid=pid))\n",
    "print('<< ' '[%dMiB] ' % get_gpu_memory(gpu=gpu, pid=pid))\n",
    "\n",
    "\n",
    "# STEP2: repeat loading/unloading a model 10 times\n",
    "print()\n",
    "print('load & infer & unload for each round:')\n",
    "print('-' * 60)\n",
    "\n",
    "for MODEL_NAME in MODEL_NAMES:\n",
    "    print('Target model:', MODEL_NAME)\n",
    "    print('Round:', end=' ')\n",
    "    \n",
    "    print('[%dMiB] -> ' % get_gpu_memory(gpu=gpu, pid=pid), end='')\n",
    "    \n",
    "    for i in range(10):\n",
    "        print(i, end='')\n",
    "        !curl -X POST $URL/v2/repository/models/$MODEL_NAME/load\n",
    "        \n",
    "        # warmup\n",
    "        function_name = MODEL_NAME + '_infer'\n",
    "        globals()[function_name](no_output=True)\n",
    "        \n",
    "        !curl -X POST $URL/v2/repository/models/$MODEL_NAME/unload\n",
    "        \n",
    "        print(' -> [%dMiB] -> ' % get_gpu_memory(gpu=gpu, pid=pid), end='')\n",
    "    print('\\n')\n",
    "\n",
    "print('-' * 60)\n",
    "print('Final state:')\n",
    "print(json.dumps(get_model_index(), indent=4))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89965334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e3b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba0e55ae",
   "metadata": {},
   "source": [
    "### Stability Tests / Load & Unload Tests / Many Models, One Version per Model-2\n",
    "> repository 放置多種模型(只含一個 version)\n",
    "> - 依序 load / unload 每一種模型, (註: Server 同時間只會載入一種模型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "862d276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input gpu-index, i.e. [0|1|2|3]: 0\n",
      "Please input gpu-pid: 718066\n",
      "Current GPU memory: 199MiB\n",
      "\n",
      "MODEL_NAMES: ['add_sub', 'inception_graphdef', 'resnet18_onnx']\n",
      "\n",
      "------------------------------------------------------------\n",
      ">> [199MiB] \n",
      "Round-0: [199MiB] -> add_sub -> [273MiB] -> inception_graphdef -> [4391MiB] -> resnet18_onnx -> [4411MiB] -> \n",
      "Round-1: [4411MiB] -> add_sub -> [4411MiB] -> inception_graphdef -> [4411MiB] -> resnet18_onnx -> [4411MiB] -> \n",
      "Round-2: [4411MiB] -> add_sub -> [4411MiB] -> inception_graphdef -> [4411MiB] -> resnet18_onnx -> [4411MiB] -> \n",
      "Round-3: [4411MiB] -> add_sub -> [4411MiB] -> inception_graphdef -> [4411MiB] -> resnet18_onnx -> [4411MiB] -> \n",
      "Round-4: [4411MiB] -> add_sub -> [4411MiB] -> inception_graphdef -> [4411MiB] -> resnet18_onnx -> [4411MiB] -> \n",
      "Round-5: [4411MiB] -> add_sub -> [4411MiB] -> inception_graphdef -> [4411MiB] -> resnet18_onnx -> [4411MiB] -> \n",
      "Round-6: [4411MiB] -> add_sub -> [4411MiB] -> inception_graphdef -> [4411MiB] -> resnet18_onnx -> [4411MiB] -> \n",
      "Round-7: [4411MiB] -> add_sub -> [4411MiB] -> inception_graphdef -> [4411MiB] -> resnet18_onnx -> [4411MiB] -> \n",
      "Round-8: [4411MiB] -> add_sub -> [4411MiB] -> inception_graphdef -> [4411MiB] -> resnet18_onnx -> [4411MiB] -> \n",
      "Round-9: [4411MiB] -> add_sub -> [4411MiB] -> inception_graphdef -> [4411MiB] -> resnet18_onnx -> [4411MiB] -> \n",
      "<< [4411MiB] \n",
      "------------------------------------------------------------\n",
      "Final state:\n",
      "[\n",
      "    {\n",
      "        \"name\": \"add_sub\",\n",
      "        \"version\": \"1\",\n",
      "        \"state\": \"UNAVAILABLE\",\n",
      "        \"reason\": \"unloaded\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"inception_graphdef\",\n",
      "        \"version\": \"1\",\n",
      "        \"state\": \"UNAVAILABLE\",\n",
      "        \"reason\": \"unloaded\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"resnet18_onnx\",\n",
      "        \"version\": \"1\",\n",
      "        \"state\": \"UNAVAILABLE\",\n",
      "        \"reason\": \"unloaded\"\n",
      "    }\n",
      "]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# get the following info from nvidia-smi\n",
    "gpu = input('Please input gpu-index, i.e. [0|1|2|3]: ')\n",
    "pid = input('Please input gpu-pid: ')\n",
    "print('Current GPU memory: %dMiB' % get_gpu_memory(gpu=gpu, pid=pid))\n",
    "print()\n",
    "\n",
    "MODEL_NAMES = get_model_names()\n",
    "print(\"MODEL_NAMES:\", MODEL_NAMES)\n",
    "print()\n",
    "\n",
    "print('-' * 60)\n",
    "print('>> ' '[%dMiB] ' % get_gpu_memory(gpu=gpu, pid=pid))\n",
    "\n",
    "# repeat loading/unloading each model 10 times\n",
    "for i in range(10):\n",
    "    print('Round-' + str(i) + ': ', end='')\n",
    "    print('[%dMiB] -> ' % get_gpu_memory(gpu=gpu, pid=pid), end='')\n",
    "    \n",
    "    for MODEL_NAME in MODEL_NAMES:\n",
    "        print(MODEL_NAME, end='')\n",
    "        !curl -X POST $URL/v2/repository/models/$MODEL_NAME/load\n",
    "        \n",
    "        # warmup\n",
    "        function_name = MODEL_NAME + '_infer'\n",
    "        globals()[function_name](no_output=True)\n",
    "        \n",
    "        !curl -X POST $URL/v2/repository/models/$MODEL_NAME/unload\n",
    "        \n",
    "        print(' -> [%dMiB] -> ' % get_gpu_memory(gpu=gpu, pid=pid), end='')\n",
    "    print()\n",
    "    \n",
    "print('<< ' '[%dMiB] ' % get_gpu_memory(gpu=gpu, pid=pid))\n",
    "\n",
    "print('-' * 60)\n",
    "print('Final state:')\n",
    "!sleep 1 # wait for the 'add_sub' model to be finished\n",
    "print(json.dumps(get_model_index(), indent=4))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc815ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a3803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b06d3e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-26 16:03:41.773755\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3558  100  3558    0     0   622k      0 --:--:-- --:--:-- --:--:--  694k\n",
      "nv_inference_request_success{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 2.000000\n",
      "nv_inference_request_failure{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 0.000000\n",
      "nv_inference_count{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 2.000000\n",
      "nv_inference_exec_count{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 2.000000\n",
      "nv_inference_request_duration_us{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 13917127.000000\n",
      "nv_inference_queue_duration_us{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 12000653.000000\n",
      "nv_inference_compute_input_duration_us{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 1349.000000\n",
      "nv_inference_compute_infer_duration_us{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 1914725.000000\n",
      "nv_inference_compute_output_duration_us{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 64.000000\n",
      "------------------------------------------------------------\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "------------------------------------------------------------\n",
      "2021-08-26 16:04:32.594052\n",
      "/bin/bash: fork: retry: No child processes\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3573  100  3573    0     0  43450      0 --:--:-- --:--:-- --:--:-- 43573\n",
      "nv_inference_request_success{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 1382.000000\n",
      "nv_inference_request_failure{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 0.000000\n",
      "nv_inference_count{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 1382.000000\n",
      "nv_inference_exec_count{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 664.000000\n",
      "nv_inference_request_duration_us{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 67574048.000000\n",
      "nv_inference_queue_duration_us{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 38648681.000000\n",
      "nv_inference_compute_input_duration_us{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 1578563.000000\n",
      "nv_inference_compute_infer_duration_us{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 26568001.000000\n",
      "nv_inference_compute_output_duration_us{gpu_uuid=\"GPU-f9dc5b48-214b-7431-8d67-f73f58eb3316\",model=\"inception_graphdef\",version=\"2\"} 85662.000000\n",
      "------------------------------------------------------------\n",
      "elapsed time: 50.82029700279236\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "t0 = datetime.now()\n",
    "print(t0)\n",
    "!curl localhost:9002/metrics | grep inception\n",
    "print('-' * 60)\n",
    "\n",
    "times = 1000\n",
    "for i in range(times):\n",
    "    if i % 100 == 0: print(i)\n",
    "        \n",
    "    cmd = '''\n",
    "        python3.6 \\\n",
    "            \"image_client.py\" \\\n",
    "            -m inception_graphdef \\\n",
    "            -s INCEPTION \\\n",
    "            \"/home/ocistn3/victorlw_chen/images/mug.jpg\" -x 2 \\\n",
    "            -u localhost:9000 \\\n",
    "             2>&1 > /dev/null &\n",
    "        '''\n",
    "    #!$cmd\n",
    "    os.system(cmd)\n",
    "    #os.system('sleep 0.3')\n",
    "\n",
    "print('-' * 60)\n",
    "t1 = datetime.now()\n",
    "print(t1)\n",
    "!curl localhost:9002/metrics | grep inception\n",
    "\n",
    "print('-' * 60)\n",
    "print('elapsed time:', t1.timestamp() - t0.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a1351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
